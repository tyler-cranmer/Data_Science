{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3022 Homework\n",
    "<figure>\n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Tyler Cranmer\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you referenced any web sites or solutions not of your own creation, list those references here:\n",
    "\n",
    "* List any external references or resources here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you're going to compute confidence intervals for two different datasets using multiple methods. The first data set is quite large, and you should find that the different techniques yield similar answers. The second dataset is much smaller. You will then solve a problem concerning confidence intervals for proportions (*e.g.* polling results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem #1 - Traffic Comparison [50 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The British [Luddite](https://en.wikipedia.org/wiki/Luddite) society argues that modern traffic lights cause more accidents than traditional \"give way\" (yield) intersections and that the problem is getting worse over time.\n",
    "\n",
    "Being a smart data scientist, you realize that \"correlation can't prove causation\", but you can at least determine if automatic intersections have more dangerous accidents that traditional intersections. Thinking quickly you [realize that traffic accident data is available](https://www.kaggle.com/daveianhickey/2000-16-traffic-flow-england-scotland-wales) and has been provided to you for the 2005-2007 year spand in the file 'accidents_2005_to_2007.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4e8e643262a137a0570a8c0465874ba6",
     "grade": false,
     "grade_id": "cell-1564c2981114656f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('accidents_2005_to_2007.csv', dtype='unicode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This datafile contains a number of columns [as described here](https://www.kaggle.com/daveianhickey/2000-16-traffic-flow-england-scotland-wales/data). The data has a number of columns of interest. However, becasue the data set is large and somewhat unclean, you'll need to convert columns of interest into the appropriate data types. You should convert the following columns to their appropriate data types:\n",
    "* Number_of_Casualties\n",
    "* Junction_Control\n",
    "* Year\n",
    "\n",
    "Modify the dataframe `df` to have these columns converted to the appropriate types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "93abc24f281585fa0823e8cab05f275b",
     "grade": true,
     "grade_id": "cell-9883abe795cf7d97",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "df = pd.DataFrame(df)\n",
    "df['Number_of_Casualties'] = df['Number_of_Casualties'].astype('int64')\n",
    "df['Junction_Control'] = df['Junction_Control'].astype('str')\n",
    "df['Year'] = df['Year'].astype('int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Junction_Control' has a single machine-controlled traffic mechanism, 'Automatic traffic signal'. The other methods are all human controlled. Create a new column labeled `machine` that distinguishes the accidents involving automatic traffic light controls from the other accidents (e.g. stop sign, yield, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "64fb77e103bbcd56ed2e9e16bd709d6a",
     "grade": true,
     "grade_id": "cell-43022efe5d5c2af3",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            other\n",
       "1         Automatic traffic signal\n",
       "2                            other\n",
       "3                            other\n",
       "4                            other\n",
       "                    ...           \n",
       "570006                       other\n",
       "570007                       other\n",
       "570008                       other\n",
       "570009                       other\n",
       "570010                       other\n",
       "Name: machine, Length: 570011, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "list = []\n",
    "for row in df['Junction_Control']:\n",
    "        list.append(row)\n",
    "\n",
    "\n",
    "df['machine'] = list\n",
    "df['machine'] = df['machine'].replace(['Stop Sign','Giveway or uncontrolled', 'Authorised person', 'nan'],'other')\n",
    "df['machine'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Luddite Arugment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first show the data group by year and type of intersection or junction control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6360409aefc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'machine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.groupby(['machine', 'Year']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Luddite argument is that machine controlled intersections have a higher mean number of casulaties every year. In other words, in 2005, 1.362 is less than 1.377 and so one.\n",
    "\n",
    "You argue that the table provides one estimate of the mean, and we'd really like to have an *interval estimate* -- it may be that some values were not reported, or there was mis-reporting or that values will differ in others years.\n",
    "\n",
    "One way to compare this is to conduct a *hypothesis test*, and we will do that later in the semester. For now, we're going to compare our interval estimates for the mean with machines and without and if they overlap, we'll agree that the machine factor has no impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine accident casualties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to compare the number of accident casulaties in our data from machine controlled and non-machine controlled intersections. We'll first examine the data for the non-machine controlled intersections.\n",
    "\n",
    "Create a vector `data_nonm` of the number of casualties for non-machine controlled intersections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "694c4fd2d9a2e168602b159cb0dc5bbe",
     "grade": true,
     "grade_id": "cell-46f005ea600fd0e4",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "non_machine_controlled = df[df['machine'] == 'other']\n",
    "data_nonm = non_machine_controlled['Number_of_Casualties']\n",
    "data_nonm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the mean of the casulaties at non-machine intersections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data_nonm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a similar set of data for machine controlled intersections called `data_m`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "086940ad6a4a0adef8b08781cc61e2fc",
     "grade": true,
     "grade_id": "cell-addc212bc2b58693",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "machine_controlled = df[df['machine'] == 'Automatic traffic signal']\n",
    "data_m = machine_controlled['Number_of_Casualties']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the mean of that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the difference between in the means of`data_m` and `data_nonm`. It should be small but positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.mean(data_m) - np.mean(data_nonm)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Reality Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two data sets have very similar means, and the value of the means should indicate that the Luddites have a point. This rocks you to your machine-loving core and you decide to make certain reality is still in your grasp.\n",
    "\n",
    "You decide to first determine that the data for the non-machine casualties is not some how violating Markov's Inequality. That is, you seek to prove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('P(X > 2*', np.mean(data_nonm), '| < 1/2 )')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the relationship with in the form $P(X > t E[X]) = l $ for $t=2$ and substituting $E[X]$ and $l$ with the values from the data. Then print out True or False if the Markov inequality would be met using an equation involving the values for  $l$ and $t$. In other words, you evaluate an expression involving `data_nonm` and $t$ that yields True or False. You may use intermediate computations if that simplifies your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "05219d780cb8dfc5fa2c53f7b6305aa3",
     "grade": true,
     "grade_id": "cell-eb7460005d201722",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "exp_mean = 2*np.mean(data_nonm)\n",
    "\n",
    "a = 0\n",
    "b = len(data_nonm)\n",
    "for row in data_nonm:\n",
    "    if row > exp_mean:\n",
    "        a += 1\n",
    "c = a /(b)\n",
    "\n",
    "if c < .5:\n",
    "    print('The Markov inequality would stand true\\n')\n",
    "    print(c) \n",
    "else:\n",
    "    print('False')\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perhaps it's a fluke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a chance that the difference in the means is a fluke and that the varability of the data changes the conclusion that the machines are killing people. \n",
    "\n",
    "You decide to use Chebeyshev's inequality to determine if $P(|X-E[X]| > \\delta) \\leq 0.05%$ where $\\delta$ is the difference between the mean of `data_m` and `data_nonm_`. In other words, you're trying to determine $P(|X-E[X]| < \\delta) \\geq 0.95$ for the `data_m` data -- that 95% of the `data_m` samples are within $\\delta$ of the sample mean. If that's true, then you believe (perhaps incorrectly) that there's strong evidence that the means are \"statistically identical\".\n",
    "\n",
    "Print out the expression for Chebyshev's inequality, $P(|X-E[X]| < \\delta) = p$, substituting $\\delta$ for the difference of the means and $p$ with the actual probability that the differences are less than $\\delta$. \n",
    "\n",
    "Your output should be formated like `P(|X-E[X]|] <  1.2345 ) = 1.2345` but using the proper bound and actual percentage of the values for which the bound is true.\n",
    "\n",
    "Then, print True or False if Chebyshev's inequality holds given the sample mean and variance, $\\delta$ and $p$. In other words, you shoudl evaluate a conditional expression involving the $p$ computed above, the variance of the data and $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2c96b53f5ad0714b87c908b33a67140c",
     "grade": true,
     "grade_id": "cell-78f57f62bb90e6e5",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "expected_mean_m = np.mean(data_m)\n",
    "sigma = np.mean(data_m) - np.mean(data_nonm)\n",
    "\n",
    "\n",
    "a = 0\n",
    "b = len(data_m)\n",
    "for row in data_m:\n",
    "    if abs(row - expected_mean_m) < sigma:\n",
    "        a+=1\n",
    "        \n",
    "        \n",
    "prob = a/b\n",
    "\n",
    "print('P(|X-E[X]| < ', sigma, ') = ', prob, '\\n')\n",
    "\n",
    "if prob < .05:\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perhaps it's normal to be doubtful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things are looking bad for the machine lovers; to date, there's every reason to believe that machines at intersections cause more casualities thatn non-machine methods. The means are similar, but machines have slightly more casulaties. It's not likely that the difference between machines and non-machine casualties is so small that it's insigificant according to Chebyshev's inequality. \n",
    "\n",
    "You recognize that using *confidence intervals* may provide a solution. If the confidence intervals don't overlap, it's clear the means are different; if they do, you're willing to believe they're similar (again, perhaps incorrectly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You recall that the Central Limit Theorem says that as $n \\rightarrow \\infty$, $\\sqrt{n}\\frac{\\bar X_n - \\mu}{\\sigma}$ has a standard normal distribution. This leaves you confused, because when you look at distribution of data using a quantile-quantile plot, the number of casulaties is decidely **not** normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(data_m,plot=plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You consult your guru who points out you've misunderstood the CLT. The CLT says that *samples of the estimate of mean* will be normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct `data_m_ests` as 1000 samples of the mean of the `data_m` data using the same procedure we used in class for *bootstrapping* your data. This will involve the `np.random.choice` function. Each estimate of the mean should involve at least 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m_ests = clt(data_m,1000,1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "73070da0f948f4521eff25a3a17e8ae5",
     "grade": true,
     "grade_id": "cell-81f3eb95efa2e5ed",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "data_m_ests = np.random.choice(data_m,1000)\n",
    "\n",
    "# print(np.mean(data_m_ests))\n",
    "\n",
    "# print(np.mean(data_m))\n",
    "# print(data_m_ests)\n",
    "# data_m_ests = clt(data_m,1000,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the histogram and the QQplot of the `data_m_ests` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_m_ests);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(data_m_ests,plot=plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the confidence interval using the Student-T distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You recall that it's usually safer to use the Student-T distribution if the underlying dataset is not normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Student-T distribution, compute the 95% confidence interval for both `data_m` and `data_nonm`. Print out the lower (2.5%), the mean and the upper (97.5%) limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d7eb47d2873d6372223ebb71be7c335a",
     "grade": true,
     "grade_id": "cell-2d829ca1ff8f542c",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "means = ( [ np.mean( np.random.choice(data_m, data_m.size) ) for x in range(50) ] )\n",
    "data_m_range = np.percentile(means, [2.5, 97.5])\n",
    "print('data_m has mean', np.mean(data_m), 'with lower CI', data_m_range[0], 'and upper CI', data_m_range[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0b59409985e9d29293d14cdb698ab1b5",
     "grade": true,
     "grade_id": "cell-54c2cb1e6981e62d",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "nonm_means = ( [ np.mean( np.random.choice(data_nonm, data_nonm.size) ) for x in range(50) ] )\n",
    "data_nonm_range = np.percentile(nonm_means, [2.5, 97.5])\n",
    "print('data_nonm has mean', np.mean(data_nonm), 'with lower CI', data_nonm_range[0], 'and upper CI', data_nonm_range[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the confidence intervals overlap? I.e. is there an overlap in the range of estimates of $\\bar x_n$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7f170e60964f573f6c80d7b28c1ed244",
     "grade": true,
     "grade_id": "cell-4473a13389a97553",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Yes the confidence intervals and estimates of Xn overlap. They are almost identical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Confidence Interval Using Large Sample Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You suddenly realize that you have have a lot of data and it's likely that the large data assumption can hold and you can use the standard normal rather than the Student-T distribution. You decide to do that to see if it changes the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the standard normal distribution, compute the 95% confidence interval for both `data_m` and `data_nonm`. Print out the lower (2.5%), the mean and the upper (97.5%) limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xbar = np.array([np.mean(np.random.choice(data_m, 1000)) for x in range(1000)])\n",
    "mu=np.mean(data_m)\n",
    "sigma = Xbar.std()\n",
    "\n",
    "z = (Xbar - mu)/sigma\n",
    "z = np.percentile(Xbar, [2.5, 97.5])\n",
    "print('data_m has mean', np.mean(data_m), 'with lower CI', z[0], 'and upper CI', z[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b484ecbe22805955a93d00894e7781b7",
     "grade": true,
     "grade_id": "cell-bb868f5925041ea0",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "nonm_Xbar = np.array([np.mean(np.random.choice(data_nonm, 1000)) for x in range(200000)])\n",
    "nonm_mu=np.mean(data_nonm)\n",
    "nonm_sigma = nonm_Xbar.std()\n",
    "\n",
    "nonm_z = (nonm_Xbar - nonm_mu)/nonm_sigma\n",
    "nonm_z = np.percentile(nonm_Xbar, [2.5, 97.5])\n",
    "print('data_m has mean', np.mean(data_nonm), 'with lower CI', nonm_z[0], 'and upper CI', nonm_z[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "31d0ec658e75bc3fd793636f1abaacbc",
     "grade": true,
     "grade_id": "cell-4e1d852e1ae414f9",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code her "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this change the results of your comparisons of the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "84d84a42f84fba83518f66db8414ebcb",
     "grade": true,
     "grade_id": "cell-191e812cedf201dd",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to leave no stone un-turned, you decide to determine bootstrap confidence intervals to see if they're consistent with the prior confidence intervals.\n",
    "\n",
    "To do this, it will be easiest to write a function `tstar(x,m)` that computes the bootstrap `t*` value for sample `x` and overall mean `m`. By this time in the class, we're going to assume you have learned how to write a simple function.\n",
    "\n",
    "Provide function `t`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7eec315f41d155952de3a02b4bfc7612",
     "grade": true,
     "grade_id": "cell-c227a3c5fc9eb3c7",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tstar(xi, mean):\n",
    "    se = np.std(xi, ddof=1) / np.sqrt(len(xi))\n",
    "    return np.mean(xi - mean) / se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the bootstrap confidence interval process, compute the 95% confidence interval for both `data_m` and `data_nonm`. Print out the lower (2.5%), the mean and the upper (97.5%) limits. You should also print out the lower and upper estimates $c_l$ and $c_u$ determined by the bootstrap. Use 1000 sample means of 20000 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3d750ae422680bdc2b5ffc9b4ca56e35",
     "grade": true,
     "grade_id": "cell-67152c570e6dd605",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "#code here\n",
    "data_m\n",
    "mu = np.mean(data_m)\n",
    "\n",
    "def tstar(xi, mean):\n",
    "    se = np.std(xi, ddof=1) / np.sqrt(len(xi))\n",
    "    return np.mean(xi - mean) / se\n",
    "\n",
    "t_vals = np.array([tstar(np.random.choice(data_m,10000),mu) for x in range(200000)])\n",
    "c1,c2=np.percentile(t_vals, [2.5, 97.5])\n",
    "print(\"c1 is :\", c1, \"Mean is: \", mu, \"c2 is:\",c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "990b7cd9be8ac748e4ca850383ef9428",
     "grade": true,
     "grade_id": "cell-ab18ea4c27c899c0",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "data_nonm\n",
    "dmu = np.mean(data_nonm)\n",
    "\n",
    "def tstar(xi, mean):\n",
    "    se = np.std(xi, ddof=1) / np.sqrt(len(xi))\n",
    "    return np.mean(xi - mean) / se\n",
    "\n",
    "d_vals = np.array([tstar(np.random.choice(data_nonm,1000),dmu) for x in range(200000)])\n",
    "d_c1,d_c2=np.percentile(d_vals, [2.5, 97.5])\n",
    "print(\"c1 is :\", d_c1, \"Mean is: \", dmu, \"c2 is:\",d_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the confidence intervals symmetric? I.e. is $c_l = -c_u$? Also, do your earlier conclusions about the means still hold? Since the bootstrap depends on random samples, you may want to run this a few times befor answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b95816babee65cc831bb666521bcb0f5",
     "grade": true,
     "grade_id": "cell-46d4015139840361",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The Confidence intervals are semetric and because the mean is within the two values, the mean still holds. These were the only two intervals that dont match the others, so there might be an error in my calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can I just bootstrap the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you have a fair amount of data, you decide to settle the issue using an *empirical bootstrap* to estimate the distribution of the mean (*i.e.* not a bootstrap confidence interval).\n",
    "\n",
    "For both `data_m` and `data_nonm`, compute the 95% confidence interval using a boostrap with 20,000 samples repeated 1,000 times.  For each dataset, print out the confidence interval in terms of (lower_bound, mean, upper_bound)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cc7728deb24f8e8574259d4df3c7816c",
     "grade": true,
     "grade_id": "cell-6b1297b06695d29b",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "def boot(x):\n",
    "    mean = np.mean(x)\n",
    "    boot_means = ([np.mean(np.random.choice(x,1000))for i in range(200000)])\n",
    "    c1,c2=np.percentile(boot_means, [2.5, 97.5])\n",
    "    return c1,mean,c2\n",
    "boot(data_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "861d55fc7800703ff075de9f9fb6252a",
     "grade": true,
     "grade_id": "cell-896e8c4b28bad3c9",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "def boot(x):\n",
    "    mean = np.mean(x)\n",
    "    boot_means = ([np.mean(np.random.choice(x,1000))for i in range(200000)])\n",
    "    c1,c2=np.percentile(boot_means, [2.5, 97.5])\n",
    "    return c1,mean,c2\n",
    "boot(data_nonm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the conclusions change compared to the previous methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e35867f18aef07392527f6733cc1bf73",
     "grade": true,
     "grade_id": "cell-063b52b325bd3189",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Yes, these values were slightly different to the bootstrap method from before, which makes me think that my bootstrap formula was probably off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, you'll find a table into which you should enter your lower mean estimate, mean and upper mean estimate for each of the indicated techniques.\n",
    "\n",
    "Round each result to four siginificant digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dee02145b128d3ff06ca8aa60b064711",
     "grade": true,
     "grade_id": "cell-5d4d3053c6843af7",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "| Method | Dataset | $c_l$ | $\\bar x_n$ | $c_u$ |\n",
    "| :---:  | :------:|:-----:|:----------:|:----:|\n",
    "| T-dist | m | 1.3663 | 1.3693  | 1.3756  | \n",
    "| T-dist | nonm | 1.3604 | 1.3628  | 1.36435 |\n",
    "| Normal | M | 1.3179  | 1.3693 | 1.424 |\n",
    "| Normal | nonm | 1.314 | 1.3628 | 1.4130|\n",
    "| Boot CI | m | -1.9017 | 1.3693  | 1.856 |\n",
    "| Boot CI | nonm | -2.0977 | 1.3628 | 1.9578 |\n",
    "| Emp. Boot | m | 1.321 | 1.3683 | 1.423 |\n",
    "| Emp. Boot | nonm | 1.313 | 1.362 | 1.416 |\n",
    "\n",
    "YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine this table and comment on the relationship between the confidence intervals from the T-distribution and the Normal distribution. Are they similar? Also comment on the relationship between the Normal distribution and the Bootstrap CI. Lastly, comment on the Bootstrap CI and the Empirical Bootstrap estimate; before answering the question, go back to your Empirical Bootstrap and draw much larger samples (e.g. 1000 samples of 200,000 rather than 20,000). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "368c50d2167174154c8fb289bd9a72cb",
     "grade": true,
     "grade_id": "cell-e027c6723a60c432",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The T-distribution and the Normal Distribution are very similar in thier results but my bootstrap CI was off compared to the other calculations. the normal distribution compared to the emperical bootstrap was relatively close to eachother. Almost identical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem #2 - Speed of light [25 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now address a smaller problem where we'll see more stark differences between the techniques.\n",
    "\n",
    "We're going to use a reduced version of the the Michelson speed of light data. Because the Michelson data has 100 elements, we would find that the Student-T and Standard Normal distibution would give almost identical results. Although that's not \"big data\", it's useful to understand the difference between the Student-T and normal distribution in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spd = np.array([  850.,   740.,   900.,  1070.,   930.,   850.,   950.,   980.,\n",
    "         980.,   880.,  1000.,   980.,   930.,   650.,   760.,   810.,\n",
    "        1000.,  1000.,   960.,   960.,   960.,   940.,   960.,   940.,\n",
    "         880.,   800.,   850.,   880.,   900.,   840.,   830.,   790.,\n",
    "         810.,   880.,   880.,   830.,   800.,   790.,   760.,   800.,\n",
    "         880.,   880.,   880.,   860.,   720.,   720.,   620.,   860.,\n",
    "         970.,   950.,   880.,   910.,   850.,   870.,   840.,   840.,\n",
    "         850.,   840.,   840.,   840.,   890.,   810.,   810.,   820.,\n",
    "         800.,   770.,   760.,   740.,   750.,   760.,   910.,   920.,\n",
    "         890.,   860.,   880.,   720.,   840.,   850.,   850.,   780.,\n",
    "         890.,   840.,   780.,   810.,   760.,   810.,   790.,   810.,\n",
    "         820.,   850.,   870.,   870.,   810.,   740.,   810.,   940.,\n",
    "         950.,   800.,   810.,   870.]) + 299000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is approximately normal because the deviation of the values represent measurement error in a constrained environment while measuring the speed of light."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to select a subset of data values to be dataset. You'll be running this code repeatedly to answer a question below after changing the size of the dataset. I've prepared a random vector of locations using `np.random.choice` that we'll use to extract the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = np.random.choice(range(100), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = pd.Series( spd[indicies] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate `speed_x` where x is `m`, `v`, `n` and `se` representing the mean, variance, sample size and *standard error*. The standard error is the quantity $\\sqrt{Var[X]/n} = \\sigma/\\sqrt{n}$, which is used computing confidence intervals using either the T-distribution or the standard normal approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_m = np.mean(speed)\n",
    "speed_v = np.var(speed)\n",
    "speed_n = len(speed)\n",
    "speed_se = np.sqrt(speed_v / speed_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a 3-tuple or 3-list named `t_ci` that contains the lower confidence limit, mean and upper confidence limit using a Student-T distribution for a 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ff579748f97fc6895cbdcd7ba1254c47",
     "grade": true,
     "grade_id": "cell-b2dd41a51e252bd6",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for a variable named `n_ci` for the confidence interval using the standard normal assumption. Although the data is approximately normally distributed, the standard normal assumption is questionable because the sample size is small (10) and the data deviates a little from normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "83074debf2f75af22660ed2b4643d858",
     "grade": true,
     "grade_id": "cell-68589eff156b5b80",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, draw a kernel density estimate (KDE) plot of the data; plot the mean using a verticle redline (using `plt.axvline`) and then the Student-T confidence interval in blue and the standard normal confidence interval in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9492dea05b48fa1d93c2d718377d476b",
     "grade": true,
     "grade_id": "cell-19fb09c777125336",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code repeatedly starting with 10 samples until you find the smallest value where you can no longer distinguish the difference between the Student-T distribution and the standard normal from the plot. Enter that number below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ddf94534e53531aea9aefb8cc64eb06e",
     "grade": true,
     "grade_id": "cell-d9ced9acd5acd357",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence limits of polling data [25 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*N.B.* Although this problem involves political polling data, I want to assure you that we're just looking for a good dataset for polling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website [538](https://projects.fivethirtyeight.com/trump-approval-ratings/?ex_cid=rrpromo) runs a series of \"weighted\" polls to estimate e.g. the popularity of the current president or other electorial outcomes.\n",
    "\n",
    "They include a number of polls. One of them, the [Feb 25-March 1 2018 IPSO poll](http://polling.reuters.com/#poll/CP3_2/), shows an approval rating of 38.2%, with a reported range of 35.6%-40.9% with a sample size of 1488. This means that $n*\\hat p = 568$ people of 1488 reported approval.\n",
    "\n",
    "There's no mention of how the estimate range 35.6%-40.9% is determined. Let's see if we can reverse engineeer the different methods we've heard about and see which they use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1488\n",
    "support = 568\n",
    "pest = support/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wilson estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the \"Wilson method\" described in the text, report the 95% confidence interval. You can either work out the solution long hand and then enter the calculations below to determine the confidence interval or you [can cheat and use the SymPy package to solve the equations](http://docs.sympy.org/latest/modules/solvers/solvers.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "51d47decdcef28834c2e8e7df1e0522e",
     "grade": true,
     "grade_id": "cell-7a2c05096306603e",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal approximation with specified $\\hat p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central limit theorem says that estimates for $S_n = X_1 + \\ldots + X_n$ should follow a standard normal distribution with mean $E[S_n]/n = p$ and variance $Var[S_n] = Var[X]/n$ or $\\sigma = \\frac{\\sqrt{p(1-p)}}{n}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps Reuter's uses the specified value of $\\hat p$ in order to determine the variance or standard deviation? Run the numbers to compute the 95% confidence interval using the standard normal approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7f3753f8d55af040037ed5c4ddb8f47f",
     "grade": true,
     "grade_id": "cell-000d03cb89397d6d",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student-T approximation with specified $\\hat p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps they're using the Student-T distribution instead of the standard normal despite the large sample size? Give that a try and compute the 95% confidence interval using the student-T distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4f46ef951b21a09c093383a4dc05b31b",
     "grade": true,
     "grade_id": "cell-aae4aa81ce4eab0c",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst case variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in lecture, if we don't know the precise value of $\\hat p$, it's difficult to estimate the varince since that depends on $\\hat p$; the Wilson method is precise but annoyingly complex.\n",
    "\n",
    "However, because we know that the variance is maximal at $p=1/2$ we can use that to compute a conservative bound. Use that approximation and the standard normal assumption to compute a bound on the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "00bfb131856b2c63b47f68a84135d6f4",
     "grade": true,
     "grade_id": "cell-8532297ffe0b0b87",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give it your best guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What method do you think Reuter's used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ff8da588e915fa7712714e8d7a2a729a",
     "grade": true,
     "grade_id": "cell-e195eaea201ceb3c",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-test data_nonm\n",
    "\n",
    "data_nonm\n",
    "dmu = np.mean(data_nonm)\n",
    "\n",
    "def tsample(xi, mean):\n",
    "    se = np.std(xi, ddof=1) / np.sqrt(len(xi))\n",
    "    return np.mean(xi - mean) / se\n",
    "\n",
    "d_vals = np.array([tsample(np.random.choice(data_nonm,50),dmu) for x in range(1000)])\n",
    "c1,c2=np.percentile(d_vals, [2.5, 97.5])\n",
    "print(\"c1 is :\", c1, \"Mean is: \", mu, \"c2 is:\",c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-test data_m\n",
    "\n",
    "#code here\n",
    "data_m\n",
    "mu = np.mean(data_m)\n",
    "\n",
    "def tsample(xi, mean):\n",
    "    se = np.std(xi, ddof=1) / np.sqrt(len(xi))\n",
    "    return np.mean(xi - mean) / se\n",
    "\n",
    "t_vals = np.array([tsample(np.random.choice(data_m,50),mu) for x in range(1000)])\n",
    "c1,c2=np.percentile(t_vals, [2.5, 97.5])\n",
    "print(\"c1 is :\", c1, \"Mean is: \", mu, \"c2 is:\",c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

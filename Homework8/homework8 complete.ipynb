{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8d_I7GkRsZS"
   },
   "source": [
    "# CSCI 3022 Homework\n",
    "<figure>\n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n--bf0P5RsZS"
   },
   "outputs": [],
   "source": [
    "NAME = \"Tyler Cranmer\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS0ZwUwqRsZS"
   },
   "source": [
    "If you referenced any web sites or solutions not of your own creation, list those references here:\n",
    "\n",
    "* List any external references or resources here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8Gzida2RsZS"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lncfNJq8RsZS"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import patsy\n",
    "import sklearn\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import sklearn.neighbors as neighbors\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Za-jalmORsZS"
   },
   "source": [
    "# Problem 1 [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7qGJ8R3RsZS"
   },
   "source": [
    "Logistic regression uses the *logit* function to assign probabilities to predicted values. The parameters of the logit function are determined using maximum liklihood estimation. In certain cases, logistic classification fails to classify data we would consider \"easy\" to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhCoyfb8RsZS"
   },
   "source": [
    "Create two samples of data, each of 3000 samples drawn from a $N(30,10)$ distribution that is then limited to a lower range of 5 and upper range of 50 (*i.e.* using np.min/np.max). These samples represent the ages of a random population. Call the first sample `age18` and create a second set named `age0` that is `age18-18`. Then create a vector `is_adult` that is `1` for each `age18` entry that is $\\geq 18$ and 0 for all other entries.\n",
    "\n",
    "In other words, both samples contain the same data, but `age0` is shifted by `18`. The `is_adult` vector encodes the same information for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMf0-JXwRsZS",
    "outputId": "ffcab13e-ea0e-4d6c-8050-e43b9a4bd9c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age18\n",
      "      is_adult        Age\n",
      "0            1  27.905863\n",
      "1            1  30.535660\n",
      "2            0  15.482258\n",
      "3            1  26.064895\n",
      "4            1  29.899155\n",
      "...        ...        ...\n",
      "2995         1  37.120119\n",
      "2996         0  12.861760\n",
      "2997         1  22.235918\n",
      "2998         1  37.343189\n",
      "2999         1  33.145086\n",
      "\n",
      "[3000 rows x 2 columns]\n",
      "\n",
      "\n",
      "\n",
      "Age0\n",
      "      is_adult        Age\n",
      "0            1  21.176086\n",
      "1            1  12.750618\n",
      "2            1   1.431912\n",
      "3            1   3.782008\n",
      "4            1  19.234487\n",
      "...        ...        ...\n",
      "2995         1  15.467637\n",
      "2996         1   3.460003\n",
      "2997         1   7.855395\n",
      "2998         1   6.535741\n",
      "2999         0  -2.982209\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "mu = 30\n",
    "sigma = 10\n",
    "\n",
    "N_30_10 = np.random.normal(mu,sigma,1000)\n",
    "df = pd.DataFrame(N_30_10, columns = ['Age'])\n",
    "df = df[df['Age'] >= 5]\n",
    "df = df[df['Age'] <= 50]\n",
    "\n",
    "n_30_10 = np.random.normal(mu,sigma,1000)\n",
    "df2 = pd.DataFrame(n_30_10, columns = ['Age'])\n",
    "df2 = df2[df2['Age'] >= 5]\n",
    "df2 = df2[df2['Age'] <= 50]\n",
    "\n",
    "\n",
    "age18 = np.random.choice(df['Age'],3000)\n",
    "age18 = pd.DataFrame(age18, columns = ['Age'])\n",
    "age18['is_adult'] = np.where(age18['Age'] >= 18, 1 , 0)\n",
    "col_name='is_adult'\n",
    "first_col = age18.pop(col_name)\n",
    "age18.insert(0, col_name, first_col)\n",
    "\n",
    "age0 = np.random.choice(df2['Age'],3000)\n",
    "age0 = pd.DataFrame(age0, columns = ['Age'])\n",
    "age0['is_adult'] = np.where(age0['Age'] >= 18, 1 , 0)\n",
    "col_name='is_adult'\n",
    "first_col = age0.pop(col_name)\n",
    "age0.insert(0, col_name, first_col)\n",
    "age0['Age'] = age0['Age'] - [18]\n",
    "\n",
    "print(\"Age18\")\n",
    "print(age18)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Age0\")\n",
    "print(age0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qShJWbjNRsZT"
   },
   "source": [
    "Fit a logistic regression to the `age18` and `is_adult` dataset and print out the confusion matrix resulting from predicting the results for the `age18` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXWXGotvRsZT",
    "outputId": "350f9e2d-56c4-409d-a9ad-e45663e711f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age18 Confusion Matrix\n",
      " [[ 350    5]\n",
      " [   0 2645]]\n",
      "\n",
      "Score is 0.9983333333333333\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "y,X = patsy.dmatrices(\"is_adult ~ 0+Age\", data=age18)\n",
    "\n",
    "lr = sklearn.linear_model.LogisticRegression()\n",
    "age18_mod = lr.fit(X,y.ravel())\n",
    "\n",
    "yhat = age18_mod.predict(X)\n",
    "yhat_p = age18_mod.predict_proba(X)\n",
    "\n",
    "print(\"Age18 Confusion Matrix\\n\",sklearn.metrics.confusion_matrix(yhat, y.ravel()))\n",
    "print('\\nScore is', age18_mod.score(X,y.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-ugLNFdRsZT"
   },
   "source": [
    "Fit a logistic regression to the `age0` and `is_adult` dataset and print out the confusion matrix resulting from predicting the results for the `age0` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_CS8XgFgRsZT",
    "outputId": "01f9b0e7-8410-4d8a-bf2d-8d210b1544d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age0 Confusion Matrix\n",
      " [[ 316    0]\n",
      " [   0 2684]]\n",
      "\n",
      "Score is 1.0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "y2,X2 = patsy.dmatrices(\"is_adult ~ 0+Age\", data=age0)\n",
    "\n",
    "lr2 = lr = sklearn.linear_model.LogisticRegression()\n",
    "age0_mod = lr2.fit(X2,y2.ravel())\n",
    "\n",
    "yhat2 = age0_mod.predict(X2)\n",
    "yhat_p2 = age0_mod.predict_proba(X2)\n",
    "\n",
    "print(\"Age0 Confusion Matrix\\n\",sklearn.metrics.confusion_matrix(yhat2, y2.ravel()))\n",
    "\n",
    "print('\\nScore is', age0_mod.score(X2,y2.ravel()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXdRHIxIRsZT"
   },
   "source": [
    "Examine the two confusion matricies, comment on their relationship to each other and their ability to predict the target data. Note that since the sample data is stochastic, you may want to run the code a few times to determine a common pattern between the confusion matricies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwolUfcmRsZT"
   },
   "source": [
    "The two confusion matrixes are the same because they are being pulled from the same data but Age0 is Age18-18. for predicting the ages of the population, the logistic regression model does very well with correctly classifying the data. The score is almost 100%. There were 7 false positives out of the 3000 datasamples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySEM784hRsZT"
   },
   "source": [
    "To help understand these results, you will prepare two plots of the `logit` plot. In the first plot, you should plot the logitistic function using the intercept and parameter from fitting the `age18` data. In the second plot, you should plot the logitstic function using $b_0 = 0$ and varying $b_1$ from 1 to 40 --  you only need 3 or 4 values in that range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "GMXtCMlDRsZT",
    "outputId": "ec42796b-397a-4e8f-f2e4-83cabf9d240b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logistic(model, x):\n",
    "    z = np.exp(model.intercept_ + model.coef_ * x)\n",
    "    return  z / (1 + z)\n",
    "\n",
    "def logistic(model, x):\n",
    "    z = np.exp(model.intercept_ + model.coef_ * x)\n",
    "    return  z / (1 + z)\n",
    "\n",
    "# your code here\n",
    "plt.plot(X,yhat,'go');\n",
    "plt.plot(X,yhat_p[:,1],'bo');\n",
    "plt.plot(X, logistic(age18_mod, X), 'ro')\n",
    "plt.axhline(0.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "xWaHI7T1RsZT",
    "outputId": "05a54070-452b-45d8-cb44-3af90db5afe7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logistic2(b_one, x):\n",
    "    z = np.exp(0 + b_one.coef_ * x)\n",
    "    return  z / (1 + z)\n",
    "\n",
    "plt.plot(X2,yhat2,'go');\n",
    "# plt.plot(X2,yhat_p2[:,1],'bo');\n",
    "plt.plot(X2, logistic2(age0_mod, X2), 'ro')\n",
    "plt.axhline(0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro-dqw68RsZU"
   },
   "source": [
    "Using those two plots and the two confusion matricies, explain the results from the logistic classification for `age18` and `age0`. Comment on the trend shown in the second graph of the ability of the logit to separate the values less than zero from those greater than zero; what value would likely result in a perfect separate? What happens when you re-run your logistic classification using 300,000 samples rather than 3,000?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq9qHAzhRsZU"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "Both logistic classifications did a good job with classifying the ages. Each of the models had 100% accuracy. From the graphs it looks like to me that the model did not have a problem predicting the ages. Age18 did a better job predicting the ages and from the graph had an easier time seperating between 0,1. Age0 struggles alittle. When you ran the logistic classification using 300,000 samples the models was actually predicted better. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7hBVUopRsZU"
   },
   "source": [
    "# Problem 2 - Surviving the Titantic [30 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vlVaqVQfRsZU",
    "outputId": "784b0ea0-2959-479b-9f42-300d6b32a9d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "ti = pd.read_csv('https://raw.githubusercontent.com/jorisvandenbossche/pandas-tutorial/master/data/titanic.csv')\n",
    "print(ti.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_SR8WBERsZU"
   },
   "source": [
    "First, build a logistic classification model that maximizes the accuracy of predicting who survives the Titantic. You can use the Patsy tool to prepare the design matrix. The underlying survival rate is 0.40 and your model should achieve an accuracy of 0.80 or greater. You should print out your prediction accuracy.\n",
    "\n",
    "In practice, you would split your data into training and testing data, but for this first set you should train and test on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X1UNIQgcRsZU"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "def add_age(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "\n",
    "    if pd.isnull(Age):\n",
    "        if Pclass == 1:\n",
    "            return 37\n",
    "        elif Pclass == 2:\n",
    "            return 29\n",
    "        else:\n",
    "            return 24\n",
    "    else:\n",
    "        return Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d7gGL7XcRsZU"
   },
   "outputs": [],
   "source": [
    "ti['Age'] = ti[['Age','Pclass']].apply(add_age,axis=1)\n",
    "ti.drop('Cabin', axis=1,inplace=True)\n",
    "ti.drop('Name', axis=1,inplace=True)\n",
    "ti.drop('Ticket', axis=1,inplace=True)\n",
    "ti.drop('Fare', axis=1,inplace=True)\n",
    "ti.drop('Embarked', axis=1, inplace=True)\n",
    "ti.drop('PassengerId', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkwGLTHjRsZU",
    "outputId": "17037af6-44ab-442e-fcea-7c09583c7ca4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    int64\n",
       "Pclass      int64\n",
       "Sex         int64\n",
       "Age         int64\n",
       "SibSp       int64\n",
       "Parch       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ti['Sex'].replace('female', 0, inplace=True)\n",
    "ti['Sex'].replace('male',1,inplace=True)\n",
    "ti['Sex'].astype('int64', copy=False)\n",
    "ti['Age'].astype('int64', copy=False)\n",
    "ti.astype('int64').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "l5cyJR1oRsZU",
    "outputId": "3e4f6f5e-b50b-4eb1-9847-0b9d4556cc43"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch\n",
       "0         0       3    1  22.0      1      0\n",
       "1         1       1    0  38.0      1      0\n",
       "2         1       3    0  26.0      0      0\n",
       "3         1       1    0  35.0      1      0\n",
       "4         0       3    1  35.0      0      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc0K2QjcRsZU",
    "outputId": "39858486-6ca3-47a8-8686-53d609119c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is (891, 1) X is (891, 5)\n",
      "Intercept (b0) is [5.34398906]\n",
      "Logistic coefficient (b1) is [[-1.22417485 -2.65406363 -0.04297501 -0.33471459 -0.06268197]]\n",
      "\n",
      "\n",
      "\n",
      "For [ 3.  1. 22.  1.  0.] we get probability [0.90574585 0.09425415]\n",
      "Score is 0.8080808080808081\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iy,iX = patsy.dmatrices(\"Survived ~ 0 + Pclass + Sex + Age + SibSp + Parch\", data=ti)\n",
    "print('y is', iy.shape, 'X is', iX.shape)\n",
    "ti_mod = lr.fit(iX, iy.ravel())\n",
    "print('Intercept (b0) is', ti_mod.intercept_)\n",
    "print('Logistic coefficient (b1) is', ti_mod.coef_)\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "iyhat = ti_mod.predict(iX)\n",
    "iyhat_p = ti_mod.predict_proba(iX)\n",
    "print('For', iX[0], 'we get probability', iyhat_p[0])\n",
    "print('Score is', ti_mod.score(iX,iy.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42D-laNXRsZU"
   },
   "source": [
    "Next, using your existing logistic design matrix, split the dataset into a train and test subset. The training set should use the even data elements and the testing set should use the odd. [You can easily construct the even/odd sets using numpy indexing.](https://stackoverflow.com/questions/4988002/shortest-way-to-slice-even-odd-lines-from-a-python-array) We use even/odd rather than `sklearn's train_test_split` function because it produces predictable output letting us compare multiple homework solutions. In practice, you would do something more robust.\n",
    "\n",
    "Re-run your regression model and report the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaNGiH1MesRm",
    "outputId": "3b51f18c-ac1c-4c2d-8331-db3de69b5e82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score is 0.802247191011236\n"
     ]
    }
   ],
   "source": [
    "ti_train= ti[::2] #Evens\n",
    "\n",
    "ti_test = ti[1::2] #odds\n",
    "\n",
    "train_y, train_X = patsy.dmatrices(\"Survived ~ 0 + Pclass + Sex + Age + SibSp + Parch\", data=ti_train)\n",
    "\n",
    "test_y, test_X = patsy.dmatrices(\"Survived ~ 0 + Pclass + Sex + Age + SibSp + Parch\", data=ti_test)\n",
    "\n",
    "train_mod = lr.fit(train_X,train_y.ravel())\n",
    "print('Training Data Score is', train_mod.score(test_X,test_y.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MF7okL4vRsZU"
   },
   "source": [
    "Now, use the K-Nearest Neighbors classification method to predict the survival rate and print out the prediction accuracy. You should do this first using the full dataset. Vary the $k$ to find the smallest $k \\in \\{2,3,4,5\\}$ that maximizes the accuracy. Remember that you may want to use a different model than for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train x shape:  (446, 5)\n",
      "Train y shape:  (446, 1)\n",
      "Test x shape:  (445, 5)\n",
      "Test y shape:  (445, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train x shape: \", train_X.shape)\n",
    "print(\"Train y shape: \", train_y.shape)\n",
    "print(\"Test x shape: \", test_X.shape)\n",
    "print(\"Test y shape: \", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkuF2Nv3RsZU",
    "outputId": "0deeb814-60bf-45e7-add7-58ee67c6a56c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is 0.8574635241301908\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=3, weights='uniform')\n",
    "KNN_mod= clf.fit(iX, iy.ravel())\n",
    "print('Score is', KNN_mod.score(iX,iy.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m43tLIUzRsZU"
   },
   "source": [
    "Now, use the K-Nearest Neighbors classification method to predict the survival rate and print out the prediction accuracy. You should now do this **using the test & train sets**. Vary the $k$ to find the smallest $k \\in \\{1,2,3,4,5\\}$ that maximizes the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tqWiBYBBRsZU",
    "outputId": "0c7c3f55-dbc9-4a92-adf3-dcd5fc2fa851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is 0.750561797752809\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "clf_t = neighbors.KNeighborsClassifier(n_neighbors=3, weights='uniform')\n",
    "train_KNN_mod=clf_t.fit(train_X,train_y.ravel())\n",
    "cyhat = train_KNN_mod.predict(test_X)\n",
    "cyhat_p = train_KNN_mod.predict_proba(test_X)\n",
    "print('Score is', train_KNN_mod.score(test_X,test_y.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zSu37xahIE5",
    "outputId": "ef6f82dc-b95a-48be-8048-4849cd21fd7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.86      0.80       261\n",
      "         1.0       0.75      0.60      0.66       184\n",
      "\n",
      "    accuracy                           0.75       445\n",
      "   macro avg       0.75      0.73      0.73       445\n",
      "weighted avg       0.75      0.75      0.74       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y.ravel(), cyhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omp6hZOiRsZU"
   },
   "source": [
    "Skim through [this paper that analyzes the survival information concerning the titantic](https://www.sciencedirect.com/science/article/pii/0277953686900419?via%3Dihub). That paper uses z-test comparisons (because $n$ is \"large\") to compare survival rates between different groups and uses the historical Mersey investigation to attribute reasons behind the differences.\n",
    "\n",
    "Assuming that `sklearn` could easily produce effects tables such as Table 4.3 in ISLR, describe the benefit of the logistic classification technique compared to the KNN technique for a researcher such as Wayne Hall. Then, describe the benefit of KNN for other applications. Use the terminology of ISLR 2.1.1 (\"Why estimate f?\") in your discussion. Assuming similar accuracy performance, which would you use if you were trying to suggest what movie to watch rather than who survived the titantic?\n",
    "\n",
    "If you would like to compute an effects table, you can use the [`StatsModels` logit](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.html) library, but I found that it depended on an older version of `scipy.stats` and it's hard to make it work, although you can make it dump out the `pvalues` table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vl0DVHDBRsZU"
   },
   "source": [
    "Logistic regression would be a better model for Wayne Hall because we can actaully predict the probabilities of certain peoples survival rate with better accuracy, while K-nearest neighbor will only classify the labels. Logistic regression actually learns while K-nearest neighbor is considered a lazy learning method. K-nearest neighbor would be great for computer vision with images. The F1 score tells you the tests accuracy. its calculated from the precision and recal of the test. With a high f1 score, you know that your model had a balance between precision and recall. For choosing a reccomended movie, the K-nearest neighbor would be best model because it will suggest movies based on your \"neighest neighbor\" similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_i6-kbRRsZU"
   },
   "source": [
    "# Problem 3. Image Recognition using Classification [40 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDMoxGsjRsZU"
   },
   "source": [
    "In this problem, you're going to use classification methods such as Logistic, LDA and KNN to classify handwritten numbers. This problem was originally posed for the US Post Office and one of the data sets was assembled by the National Institute for Standards and Technology (NIST). The problem is now a standard problem in machine learning. We'll see that we can get ~92% accuracy for the full problem using Logistic or LDA and about 98% accuracy using KNN.\n",
    "You can also use TensorFlow to [implement a \"deep learning\" solution](https://www.tensorflow.org/versions/r1.1/get_started/mnist/pros) that achieves ~99% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qpzam9NkRsZU"
   },
   "source": [
    "## Using a Small Dataset\n",
    "\n",
    "We're going to focus on the trade-off of training time vs. prediction time and the accuracy achieved by different classification methods. We will first use a dataset that uses 1797 small 8x8 (64 pixel) images. In the code below, we load the dataset using an `sklearn` interface.\n",
    "\n",
    "In this section, your goal will be to understand how *multinomial* classification functions and how you can use outputs of (some) classification tools to understand how certain or confidence you should be in a classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULuQN7wRRsZU",
    "outputId": "33a2afa2-d167-4ff1-c7f2-32c02c9bad14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape (1797, 64)\n",
      "Label Data Shape (1797,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(\"Image Data Shape\" , digits.data.shape)\n",
    "print(\"Label Data Shape\", digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkW8voV3RsZU"
   },
   "source": [
    "Each image is encoded as values between 0 and 16. When training the data, we view the image as a 64-element set of features or factors. We can view the image by arranging it an 8x8 array and using [pyplot's `imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) routine, as we do below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "ESy381PxRsZU",
    "outputId": "640209ec-b9bb-4efe-9be0-03c0b0b9a50c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit: 5\n",
      "Image:\n",
      " [[ 0.  6. 13.  5.  8.  8.  1.  0.]\n",
      " [ 0.  8. 16. 16. 16. 16.  6.  0.]\n",
      " [ 0.  6. 16.  9.  6.  4.  0.  0.]\n",
      " [ 0.  6. 16. 16. 15.  5.  0.  0.]\n",
      " [ 0.  0.  4.  5. 15. 12.  0.  0.]\n",
      " [ 0.  0.  0.  3. 16.  9.  0.  0.]\n",
      " [ 0.  1.  8. 13. 15.  3.  0.  0.]\n",
      " [ 0.  4. 16. 15.  3.  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=33\n",
    "plt.imshow(digits.data[k].reshape(8,8), cmap=plt.cm.gray)\n",
    "print('Digit:', digits.target[k])\n",
    "print('Image:\\n', digits.data[k].reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76Z53IqNRsZU"
   },
   "source": [
    "Now, divide the `digits` dataset into a train/test split using even/odd images as before. Again, we do this to allow precise comparison o the results betwen solutions and students. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iOSHm4jzRsZV",
    "outputId": "97720258-2d3d-426a-fc7f-efdc2a71aa70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(899, 64)\n"
     ]
    }
   ],
   "source": [
    "dX_train, dX_test, dy_train, dy_test = digits.data[0::2], digits.data[1::2],digits.target[0::2], digits.target[1::2]\n",
    "print(dX_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7qGMYKsRsZV"
   },
   "source": [
    "### Small Digits using Logistic\n",
    "\n",
    "Use a logistic classifier fit with the training data to then predict the test data. Report the accuracy score and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "y6bhy4hgyodV"
   },
   "outputs": [],
   "source": [
    "\n",
    "lr = sklearn.linear_model.LogisticRegression(max_iter=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDUJE4hWRsZV",
    "outputId": "089ec61b-2036-4ca9-a211-55bb128a7d1d"
   },
   "outputs": [],
   "source": [
    "# your code hear\n",
    "train_digit_mod = lr.fit(dX_train, dy_train.ravel())\n",
    "yhat = train_digit_mod.predict(dX_test)\n",
    "yhat_p = train_digit_mod.predict_proba(dX_test)\n",
    "print('Test score is', train_digit_mod.score(dX_test, dy_test.ravel()))\n",
    "print(\"Digits dataset training Data Confusion Matrix\\n\",sklearn.metrics.confusion_matrix(dy_test.ravel(),yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww1fiOsQRsZV"
   },
   "source": [
    "Based on the test data, which pairs of digits are confused more than once? In other words, if you examine the first column, you see 2 predictions where a '0' is misclassified as a '4'; you would report this as {0,4}. Construct similar sets of confused digits for all entries confused more than 1 time. Comment on the any expected and suprising outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YazZO5GJRsZV"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "{1,4}{1,8}{1,9}\n",
    "{2,4}\n",
    "{4,7}{4,9}\n",
    "{8,2}\n",
    "{9,8}{9,5}\n",
    "\n",
    "\n",
    "Its interesting to see that the numbers that were miss classified were not also miss classified in thier inverse order. i.e {4,9} was miss classified but {9,4} was not. \n",
    "\n",
    "I also dont see any misclassification of {0,4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8V2t5HMRsZV"
   },
   "source": [
    "The digit classification problem involves a *multinomial*, or more than two levels in the outcome. By default, the `LogisticRegression` method uses a series of binomial logistic regression fits to the different outcomes of the multinomial. [The `predict_proba` routine in `LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) returns the probability of the fit to each individual possible outcome (e.g. the digits '0' through '9'). The predicted outcome (i.e. the result of `predict`) is then the outcome with the largest predicted outcome.\n",
    "\n",
    "For the two examples where the predicted digit is '4' but the actual digit is '0', plot the images corresponding to those digits and print out the results of `predict_proba` for those targets. In my solution to this, produced a vector of True/False values using element-wise comparisons and then [used `np.nonzeros`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) to extract the indicies in the test data of the \"true\" values (corresponding to the samples that matched '4' in my prediction but whose actual target was '0'). you may also want to use `np.round` to round up the `predict_proba` results to 3-4 digits, making it eaiser to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "onpjdMr4RsZV",
    "outputId": "41fcac24-6d4d-447a-e6b0-fe045862c51f"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "list = []\n",
    "x=0\n",
    "y=0\n",
    "\n",
    "for i in range(len(yhat)):\n",
    "  if yhat[i] == dy_test[i]:\n",
    "    list.append(1)\n",
    "    x += 1\n",
    "  else:\n",
    "    list.append(0)\n",
    "    y += 1\n",
    "print(\"There were \", x, \" correct classications and \", y, \" missclassifications\")\n",
    "list = np.array(list)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "result = np.where(list == 0)\n",
    "print(\"Elements with the misclassications exists at following indices\", result[0], sep='\\n')\n",
    "\n",
    "#All the values that were missclassified.\n",
    "#{1,4}{1,6}{1,8}{1,9} {2,4}{2,8}{3,7}{3,9}{4,0}{4,7}{4,8}{4,9}{5,9}{6,0}{6,5}{6,8}{7,9}{8,2}{8,3}{9,8}{9,5}\n",
    "\n",
    "k = 0\n",
    "predict = train_digit_mod.predict(dX_test)[result[0][k]]\n",
    "print(\"\\nDigit Target: \", predict)\n",
    "print(\"\\nPicture:\")\n",
    "plt.imshow(dX_test[result[0][k]].reshape(8,8), cmap=plt.cm.gray);\n",
    "\n",
    "\n",
    "predict = train_digit_mod.predict_proba(dX_test)[result[0][1]]\n",
    "np.round(predict)\n",
    "print(\"\\nprediction values are: \", np.round(predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsJ8gSo2RsZV"
   },
   "source": [
    "Using the values from `predict_prob` are both mis-classified '0' values equally likely to have been classified as a '4'? Do the probabilities of the predicted outcomes comport with your visual interpretation of the digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSpJ-a5_RsZV"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "The data doesnt reflect the question of this assignment. As for the digits that are classified wrong. The prediction values are a bit off. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIYdIElwRsZV"
   },
   "source": [
    "### Small Digits using KNN\n",
    "\n",
    "Now, using the K-Nearest Neighbors method to fit and predict the test and train data. Select $k \\in \\{1,2,3,4,5\\}$ that achieves the highest accuracy. Print the accuracy score and confusion matrix for the $k$ with the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fHN9A4xdQg6s",
    "outputId": "a32d70d9-7cc3-4e02-d76d-133be0197041"
   },
   "outputs": [],
   "source": [
    "clf = neighbors.KNeighborsClassifier(n_neighbors=1, weights='uniform')\n",
    "dtrain_KNN_mod=clf_t.fit(dX_train,dy_train.ravel())\n",
    "dyhat = dtrain_KNN_mod.predict(dX_test)\n",
    "dyhat_p = dtrain_KNN_mod.predict_proba(dX_test)\n",
    "print('Test score is', dtrain_KNN_mod.score(dX_test, dy_test.ravel()))\n",
    "print(\"Digits dataset training Data Confusion Matrix\\n\",sklearn.metrics.confusion_matrix(dy_test.ravel(),dyhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HRRb_nNRsZV"
   },
   "source": [
    "As before, based on the test data, which pairs of digits are confused more than once?  Comment on the any expected and suprising outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm26yGpBRsZV"
   },
   "source": [
    "{1,8}{9,5}\n",
    "\n",
    "Both of these misclassifications were on the previous confusion matrix for the logistic Model. so the fact that the KNN model only has two misclassifications that happened more than once is pretty neat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9uBm5VSRsZV"
   },
   "source": [
    "Selecting one pair of confused digits, print out the image and the probability estimates (`predict_proba`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "UEXw9BAV_2S4",
    "outputId": "facb771f-b25a-4886-bf89-83f0b8336cfd"
   },
   "outputs": [],
   "source": [
    "print(\"There were \", x, \" correct classications and \", y, \" missclassifications\")\n",
    "list = np.array(list)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "result = np.where(list == 0)\n",
    "print(\"Elements with the misclassications exists at following indices\", result[0], sep='\\n')\n",
    "\n",
    "#All the values that were missclassified.\n",
    "#{1,4}{1,6}{1,8}{1,9} {2,4}{2,8}{3,7}{3,9}{4,0}{4,7}{4,8}{4,9}{5,9}{6,0}{6,5}{6,8}{7,9}{8,2}{8,3}{9,8}{9,5}\n",
    "\n",
    "k = 680\n",
    "\n",
    "print(\"\\nDigit Target: \", dy_test[k])\n",
    "print(\"\\nPicture:\")\n",
    "plt.imshow(dX_test[k].reshape(8,8), cmap=plt.cm.gray);\n",
    "\n",
    "predict = train_digit_mod.predict_proba(dX_test)[list[k]]\n",
    "np.round(predict)\n",
    "print(\"\\nprediction values are: \", np.round(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "ieEjpiqNRsZV",
    "outputId": "1174205e-2320-40c6-db4d-396f2756d2f2"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# your code here\n",
    "is_same = []\n",
    "x=0\n",
    "y=0\n",
    "for i in range(len(dyhat)):\n",
    "  if dyhat[i] == dy_test[i]:\n",
    "    is_same.append(True)\n",
    "    x += 1\n",
    "  else:\n",
    "    is_same.append(False)\n",
    "    y += 1\n",
    "# print(x,y)\n",
    "# print(len(dyhat))\n",
    "is_same = np.array(is_same)\n",
    "index = np.where(is_same == False)\n",
    "# index = np.nonzero(is_same)\n",
    "# print(\"Elements with the misclassications exists at following indices\", index[0], sep='\\n')\n",
    "\n",
    "k = 14\n",
    "prob_est = dtrain_KNN_mod.predict_proba(dX_test)[index[0][k]]\n",
    "predicts = dtrain_KNN_mod.predict(dX_test)[index[0][k]]\n",
    "print(\"The probability estimate is: \", np.round(prob_est))\n",
    "print(\"The predicting digit: \", np.round(predicts))\n",
    "plt.imshow(dX_test[index[0][k]].reshape(8,8), cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAgARPf6RsZV"
   },
   "source": [
    "Comment on differences in the results of `predict_proba` between the logistic and KNN classifiers. Would the results be similar for different values of $k$ in the KNN sarch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4d1uILdRsZV"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "The KNN works much better than the Logistic regression model. The predict_proba was almost on the money eaach time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcG3dfZMRsZV"
   },
   "source": [
    "### Using the larger MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbdmFVHVRsZV"
   },
   "source": [
    "We will now use the MNIST dataset, which is the same used in the TensorFlow tutorial. This dataset is large, and contiains 70,000 images each of which are 28x28 pixels.\n",
    "\n",
    "In this section, your goal will be to understand the performance of difference classification tools and their impact on usability in an application.\n",
    "\n",
    "We first load the dataset. This may take a while the first time because the data has to be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yB1vEjuRsZV",
    "outputId": "70ca49c1-3b99-489a-f87e-3ada206462b9"
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "mnist = sklearn.datasets.fetch_openml('mnist_784', version=1) \n",
    "print(\"Image Data Shape\" , mnist.data.shape)\n",
    "print(\"Label Data Shape\", mnist.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNwUj6n5RsZV"
   },
   "source": [
    "As before, the dataset has a `data` array of 784 features or factors that can be reorganized into an image. There is also a `target` value indicating the correct digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9BVheXVbRsZV",
    "outputId": "e90861d5-b6ae-4498-d74c-9ff8eea75d6c"
   },
   "outputs": [],
   "source": [
    "k=3\n",
    "plt.imshow(np.reshape(mnist.data[k], (28,28)), cmap=plt.cm.gray, label='Digit:' + str(mnist.target[k]))\n",
    "print('values:', mnist.data[k].reshape(28,28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSS8-6AuRsZV"
   },
   "source": [
    "Again, split your data into an even/odd train/test dataset using numpy indexing. You should name the data something different than your smalled 'digits' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zfmor1D1RsZV",
    "outputId": "69acb96b-df6c-4e32-86b3-fed0c2348840"
   },
   "outputs": [],
   "source": [
    "mX_train, mX_test, my_train, my_test = mnist.data[0::2], mnist.data[1::2],mnist.target[0::2], mnist.target[1::2]\n",
    "print(mX_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujkNfP5FRsZV"
   },
   "source": [
    "Now, train a logistic regression model on the MNIST training data. You [should prefix your fit function call using the %time \"magic\" command](http://ipython.readthedocs.io/en/stable/interactive/magics.html) to measure how long the fitting process takes. \n",
    "\n",
    "This will take a long time for the default method we've been using to run logistic classification problems (like more than 30 minutes), in part because the default method fits $n$ binomial classification problems to determine the multinomial model. If you start using the standard solver (`liblinear`) and decide it's too slow, use the Kernel -> Interrupt menu to stop the evaluation.\n",
    "\n",
    "Logsitic regression uses *maximum liklihood estimation* to determine the most likely outcome. There are numerous *solvers* (see [the LogisticRegression manual](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) ) that can be used and some of them are more appropriate for large multinomial problems because they fit the data to all the outcomes in one go. Find one that doesn't take forever (some should take ~15 seconds) and fit your model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFM0aBi5VSVS"
   },
   "outputs": [],
   "source": [
    "lr2 = sklearn.linear_model.LogisticRegression(solver = 'saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSytt98eRsZV",
    "outputId": "0b980fe8-91f3-45d6-d28f-aba819d5fd6e"
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "\n",
    "%time train_mnist_mod = lr2.fit(mX_train, my_train.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiegfcSzRsZV"
   },
   "source": [
    "Now, compute the predictions and `predict_proba` for the test dataset and use %time to determine how long the predictions take. Report the accuracy score and the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMWGiWdVRsZV",
    "outputId": "78b9fbda-2615-4546-cfd2-62c6340f474f"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "%time yhat = train_mnist_mod.predict(mX_test)\n",
    "%time yhat_p = train_mnist_mod.predict_proba(mX_test)\n",
    "print('Test score is', train_mnist_mod.score(mX_test, my_test.ravel()))\n",
    "print(\"Confusion Matrix\\n\",sklearn.metrics.confusion_matrix(yhat,my_test.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VH7VG38lRsZV"
   },
   "source": [
    "Now, compute the probability scores for each outcome class using `predict_proba` and plot either a histogram or KDE plot of their values. You can use the output of `predict_proba` and then use `ravel()` to turn it into single flat array suitable for feeding to `plt.hist` or `sns.kdeplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "UIykpCB3RsZV",
    "outputId": "82b8de17-4a66-48d4-aa38-517b99158162"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "my_data = yhat_p.ravel()\n",
    "plt.hist(my_data, density=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "xPEKPeU1V20C",
    "outputId": "752eadf8-1007-4ea3-da4e-cd1177566c16"
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2t8W1nW3RsZV"
   },
   "source": [
    "Linear Discriminant Analysis is supposed to be superior for multinomial classification. Run the same classification problem using LDA and time the fitting proccess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpuSwUYdRsZV",
    "outputId": "daec70d2-f933-4457-8f4d-8248dd55820d"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "%time ilda = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWYOk3ldRsZV"
   },
   "source": [
    "Predict the outcomes and report the accuracy score and confusion matrix. Time how long it takes to run the prediction using `%time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQlme5wrRsZV",
    "outputId": "d099ecf8-9495-4eec-b3a9-09653ee091d1"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "LDA_model = ilda.fit(mX_train, my_train.ravel())\n",
    "%time LDA_hat = LDA_model.predict(mX_test)\n",
    "LDA_hatp = LDA_model.predict_proba(mX_test)\n",
    "print('Test score is', LDA_model.score(mX_test, my_test.ravel()))\n",
    "print(\"Confusion Matrix\\n\",sklearn.metrics.confusion_matrix(LDA_hat,my_test.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0b2DesSRsZV"
   },
   "source": [
    "Print the distribution of outcome probabilities from `predict_proba` using a histogram or KDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "sbSnQ2S6RsZV",
    "outputId": "6ffd4f58-8f0b-4bc3-afff-24f2edaa5f4a"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "data = LDA_hatp.ravel()\n",
    "plt.hist(data, density=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "hiaVGWRrUIZG",
    "outputId": "b2f389b9-56c5-435a-fd58-53d1f9959c00"
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wojh9bO3RsZV"
   },
   "source": [
    "Compare the distribution of probability of prediction values for Logistic and LDA classification. Comment on the differences and/or similarities of the range of values from `predict_proba` returned by each method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auXzKMakRsZV"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "The two graphs looks very similar in nature. Almost identical to me. Logistic regression just took much longer to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9xfnaGARsZV"
   },
   "source": [
    "Lastly, we're going to do the same steps using the KNN algorithm. You should use $k=1$ for the KNN method and record the fitting time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmgn83LiRsZV",
    "outputId": "9cd26932-55ce-4d57-d30b-f286ba1547aa"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "clf2 = neighbors.KNeighborsClassifier(n_neighbors=1, n_jobs=-1, weights='uniform')\n",
    "%time c2train_KNN_mod=clf_t.fit(mX_train, my_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScFCOt9VXCAg"
   },
   "outputs": [],
   "source": [
    "# clf = neighbors.KNeighborsClassifier(n_neighbors=1, n_jobs=-1, weights='uniform')\n",
    "# dtrain_KNN_mod=clf_t.fit(X_train, my_train.ravel())\n",
    "# dyhat = dtrain_KNN_mod.predict(dX_test)\n",
    "# dyhat_p = dtrain_KNN_mod.predict_proba(dX_test)\n",
    "# print('Test score is', dtrain_KNN_mod.score(dX_test, dy_test.ravel()))\n",
    "# print(\"Digits dataset training Data Confusion Matrix\\n\",sklearn.metrics.confusion_matrix(dy_test.ravel(),dyhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tczFEqN4RsZV"
   },
   "source": [
    "Now run the prediction using your KNN model. Note that this will take a long time (40 minutes?). If specify `n_jobs=-1` when you create your `KNeighborsClassifier`, then predictions will use all the cores on your computer. For example, that chnaged my 40 minute run time for the full dataset to 5 minutes.\n",
    "\n",
    "You should first run the prediction on a small test set (e.g. perhaps every 40th sample) to make certain you're doing it right. The digits of the same outcome are usually bunched together and if you just e.g.select the first 1000 items, you'll find they only belong to one output class. Once you have your code working, run it for the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1CB43FlRsZV"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "%time c2yhat = dtrain_KNN_mod.predict(mX_test)\n",
    "c2yhat_p = dtrain_KNN_mod.predict_proba(mX_test)\n",
    "print('Test score is', dtrain_KNN_mod.score(mX_test, my_test.ravel()))\n",
    "print(\"Digits dataset training Data Confusion Matrix\\n\",sklearn.metrics.confusion_matrix(my_test.ravel(),c2yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0UErN5FRsZV"
   },
   "source": [
    "### Comparison\n",
    "\n",
    "Now, compare the three methods. For each method, describe the accuracy achieved, the fitting time and the prediction time. For the Logistic and LDA model, describe how the distribution of the outcome probabilities may affect the accuracy score. Assume you're trying to apply the digit classification problem in the post-office. Which method would you use? Given that the accuracy isn't 100%, what outputs of the models could you use to improve mail sorting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykZS1Q87RsZV"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "The logistic Model had an accuracy of 91% but the fitting time was excessive. It took over 2 hours to fit the data when using the Saga Solver. I left the computer running over night. the prediction time was very quick with 536ms. So that was nice. The prediction distributions were pretty drastic between 0 and 1. There was a lot of 0s predicted. The LDA Model was extremely quick with 22 seconds to fit the data but the overall accuracy was a bit weak at 85%. the prediction time was also quick at 546 ms. The Knearest neighbor was the quickest with fitting the data at 9 seconds bt the Prediction time is still running. \n",
    "\n",
    "So far without having the knn model complete, i would choose the Logistic Regression model to classify digits in the post office since the accuracy was the highest. I do know that KNN model does pretty well with predicting images. So if my model ever completed, i believe the KNN model would probably have the best accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDFCbH0Ab39y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "hmwk8.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "nteract": {
   "version": "0.14.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
